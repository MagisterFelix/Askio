# ğŸ’¬ Chat

The `Chat` handles AI-powered chat responses using an event-driven, fully asynchronous architecture. It integrates with **Ollama** for local LLM inference and communicates with other components via **RabbitMQ** using **FastStream**.

---

## ğŸ§  Responsibilities

* Consume chat request events via RabbitMQ
* Generate responses using a local Ollama model
* Emit chat responses back to the system
* Operate in a fully asynchronous, event-driven manner

---

## âš™ï¸ Technologies

| Layer         | Tool                          |
| ------------- | ----------------------------- |
| LLM Engine    | Ollama                        |
| Messaging     | FastStream \[RabbitMQ]        |
| Configuration | Pydantic \[pydantic-settings] |
| Async Runtime | Python asyncio                |

---

## ğŸ—ï¸ Architecture

* **Event-Driven**: Listens to chat-related messages and produces responses via RabbitMQ.
* **LLM-Based**: Uses Ollama as a local large language model backend for generating responses.
* **Async-First**: All operations are asynchronous and non-blocking.
* **Lightweight**: Stateless service optimized for high-concurrency chat tasks.

---

## ğŸ“ Internal Structure

```text
chat
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py         # app package init
â”‚   â”œâ”€â”€ chat.py             # core chat logic for generating responses
â”‚   â””â”€â”€ client.py           # ollama client setup
â”œâ”€â”€ .python-version         # python version lock
â”œâ”€â”€ broker.py               # message broker setup
â”œâ”€â”€ config.py               # configuration loading
â”œâ”€â”€ Dockerfile              # container image definition
â”œâ”€â”€ main.py                 # entry point
â”œâ”€â”€ pyproject.toml          # build system and dependencies
â”œâ”€â”€ README.md               # documentation
â””â”€â”€ uv.lock                 # `uv` dependency lock file
```

---

## ğŸ“ Notes

* All communication is via RabbitMQ; no HTTP API is exposed directly.
* Designed to be plug-and-play with minimal configuration.
